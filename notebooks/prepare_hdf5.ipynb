{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to HDF5\n",
    "\n",
    "This notebook has instructions on how to prepare final HDF5 dataset. It assumes you have followed the instructions to get consolidated data in `final/` folder. Each json file has entries for that WSID indexed to a global unified index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import h5py # main dumping method\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import orjson\n",
    "\n",
    "import vaex\n",
    "import json\n",
    "from glob import glob\n",
    "from ftfy import ftfy\n",
    "from io import StringIO\n",
    "from dateparser import parse\n",
    "from collections import Counter\n",
    "# from tqdm import trange\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import math\n",
    "from datetime import timedelta, date\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\n",
    "    (\"total_precipitation\", \"mm\"),\n",
    "    (\"pressure\", \"mB\"),\n",
    "    (\"max_pressure\", \"mB\"),\n",
    "    (\"min_pressure\", \"mB\"),\n",
    "    (\"radiation\", \"KJ/m^2\"),\n",
    "    (\"temp\", \"C\"),\n",
    "    (\"dew_point_temp\", \"C\"),\n",
    "    (\"max_temp\", \"C\"),\n",
    "    (\"min_temp\", \"C\"),\n",
    "    (\"max_dew\", \"C\"),\n",
    "    (\"min_dew\", \"C\"),\n",
    "    (\"max_humidity\", \"percentage\"),\n",
    "    (\"min_humidity\", \"percentage\"),\n",
    "    (\"humidity\", \"percentage\"),\n",
    "    (\"wind_direction\", \"deg\"),\n",
    "    (\"wind_gust\", \"m/s\"),\n",
    "    (\"wind_speed\", \"m/s\")\n",
    "]\n",
    "COL_NAMES = [x[0] for x in COLUMNS]\n",
    "UNITS = [x[1] for x in COLUMNS]\n",
    "BAD_ROW_SUM = 0.0\n",
    "\n",
    "def open_csv(f, wsid):\n",
    "    with open(f, \"r\", encoding=\"iso8859_1\") as d:\n",
    "        df = pd.read_csv(\n",
    "            StringIO(\"\".join(d.readlines()[8:]).replace(\"-9999\", \"0\")),\n",
    "            sep=\";\",\n",
    "            encoding=\"iso8859_1\"\n",
    "        )\n",
    "\n",
    "    df = df.drop(\"Unnamed: 19\", axis = 1)\n",
    "    \n",
    "    if wsid:\n",
    "        local_col_names = [f\"{wsid}_{c}\" for c in COL_NAMES]\n",
    "    else:\n",
    "        local_col_names = [f\"{c}\" for c in COL_NAMES]\n",
    "    \n",
    "    df.columns = [\"date\", \"hour\"] + local_col_names\n",
    "    for col in local_col_names[2:]:\n",
    "        setattr(df, col, getattr(df, col).apply(lambda x: float(str(x).replace(\",\", \".\"))))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_files = [x for x in glob(\"final/*.json\") if (\"index\" not in x and \"wsids_ordered\" not in x)]\n",
    "print(f\"final_files: {len(final_files)}\")\n",
    "\n",
    "with open(\"final/index.json\", \"r\") as f:\n",
    "    unified_idx = json.load(f)\n",
    "print(len(unified_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gids_by_file = {}\n",
    "for i, f in zip(trange(len(final_files)), sorted(final_files)): # sorted is very important because this way we can gaurantee ordering in  array i,f in enumerate(sorted(final_files)):\n",
    "    with open(f, \"r\") as f2:\n",
    "        data = json.load(f2)\n",
    "    gids_by_file[f] = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_year = 2.5\n",
    "\n",
    "year_wise_sample_count = {}\n",
    "for min_year in np.linspace(1, 5, 9):\n",
    "    samples = [len(x) for x in gids_by_file.values()]\n",
    "    # np.mean(samples), np.median(samples)\n",
    "    useful_files = []\n",
    "    useful_files_gid = {}\n",
    "\n",
    "    for f in final_files:\n",
    "        # ignore all samples have < 2 years of data, ie < 24 * 365 * 2 samples => < 17520 samples\n",
    "        if len(gids_by_file[f]) >= 24 * 365 * min_year:\n",
    "            useful_files.append(f)\n",
    "\n",
    "    print(f\"min: {min_year} Before: {len(gids_by_file)} After: {len(useful_files)}\")\n",
    "    \n",
    "    year_wise_sample_count[min_year] = len(useful_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_year = 4.5\n",
    "\n",
    "samples = [len(x) for x in gids_by_file.values()]\n",
    "# np.mean(samples), np.median(samples)\n",
    "useful_files = []\n",
    "useful_files_gid = {}\n",
    "\n",
    "for f in final_files:\n",
    "    # ignore all samples have < 2 years of data, ie < 24 * 365 * 2 samples => < 17520 samples\n",
    "    if len(gids_by_file[f]) >= 24 * 365 * min_year:\n",
    "        useful_files.append(f)\n",
    "\n",
    "useful_files = sorted(useful_files)\n",
    "useful_wsids = [x[6:10] for x in useful_files]\n",
    "# print(f\"min: {min_year} Before: {len(gids_by_file)} After: {len(useful_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positions Meta\n",
    "\n",
    "```python\n",
    "wsmeta = pd.read_csv(\"../INMET/wsid_meta.csv\")\n",
    "wsmeta = wsmeta.T\n",
    "headers = wsmeta.iloc[0].values.tolist()\n",
    "wsmeta = wsmeta[1:]\n",
    "wsmeta.columns = headers\n",
    "wsmeta = wsmeta[wsmeta.elev != \"F\"] # corrupt data\n",
    "\n",
    "wsmeta.lat = wsmeta.lat.values.astype(float)\n",
    "wsmeta.long = wsmeta.long.values.astype(float)\n",
    "wsmeta.elev = wsmeta.elev.values.astype(float)\n",
    "\n",
    "wsm = json.loads(wsmeta.to_json(orient=\"index\"))\n",
    "\n",
    "with open(\"wsid_meta.json\", \"w\") as f:\n",
    "    f.write(json.dumps(wsm))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wsid_meta.json\", \"r\") as f:\n",
    "    wsmeta = json.load(f)\n",
    "\n",
    "wsmeta_ordered = []\n",
    "for x in useful_wsids:\n",
    "    wm = wsmeta[x]\n",
    "    wsmeta_ordered.extend([wm[\"lat\"], wm[\"long\"], wm[\"elev\"]])\n",
    "wsmeta_ordered = np.array(wsmeta_ordered)\n",
    "print(f\"wsmeta_ordered: {wsmeta_ordered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final/index.json\", \"r\") as f:\n",
    "    unified_idx = json.load(f)\n",
    "print(len(unified_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_data = []\n",
    "for k in sorted(unified_idx):\n",
    "    x = unified_idx[k]\n",
    "    mon = int(x[5:7])\n",
    "    day = int(x[8:10])\n",
    "    hrs = int(x[11:12])\n",
    "    datetime_data.append((mon, day, hrs))\n",
    "datetime_data = np.array(datetime_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schema in hdf5\n",
    "hdf = h5py.File(\"weatherGiga2.hdf5\", \"w\")\n",
    "\n",
    "hdf.create_dataset(\"wsid_meta\", shape = wsmeta_ordered.shape, dtype = 'f', data = wsmeta_ordered)\n",
    "hdf.create_dataset(\"datetime\", shape = datetime_data.shape, dtype = 'i', data = datetime_data)\n",
    "\n",
    "for _, i in zip(trange(len(unified_idx)), unified_idx):\n",
    "    grp = hdf.create_group(f\"{i}\")\n",
    "    data = np.zeros(shape = (len(useful_files), 17)).astype(np.float32)\n",
    "    grp.create_dataset(\"data\", shape = data.shape, dtype = 'f', data = data)\n",
    "    grp.create_dataset(\"mask\", shape = [data.shape[0]], dtype = 'f', data = data[:,0])\n",
    "    \n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = h5py.File(\"weatherGiga2.hdf5\", \"r+\")\n",
    "\n",
    "# sorted is very important because this way we can gaurantee ordering in  array\n",
    "uf = sorted(useful_files)\n",
    "pb1 = trange(len(uf))\n",
    "for i,f in zip(pb1, uf):\n",
    "    \n",
    "    if int(i) < 1:\n",
    "        continue\n",
    "    \n",
    "    w = useful_wsids[i]\n",
    "    pb1.set_description(f\"{f}\")\n",
    "    with open(f, \"r\") as f2:\n",
    "        data = json.loads(f2.read())\n",
    "\n",
    "    for _, gid in zip(trange(len(gids_by_file[f])), gids_by_file[f]):\n",
    "        this_idx_data = hdf[f\"{gid}\"]\n",
    "        if str(sum(data[gid][2:])) != \"nan\" and sum(data[gid][2:]) != 0:\n",
    "            try:\n",
    "                this_idx_data[\"data\"][i*17: (i+1)*17] = data[gid]\n",
    "                this_idx_data[\"mask\"][i] = 1\n",
    "            except:\n",
    "                continue\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
